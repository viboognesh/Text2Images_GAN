{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "checkpoint_path = os.path.join(\n",
    "    \"taming-transformers\", \"models\", \"vqgan_imagenet_f16_16384\", \"checkpoints\"\n",
    ")\n",
    "config_path = os.path.join(\n",
    "    \"taming-transformers\", \"models\", \"vqgan_imagenet_f16_16384\", \"configs\"\n",
    ")\n",
    "\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "os.makedirs(config_path, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_path,\"last.ckpt\")\n",
    "config_path = os.path.join(config_path, \"model.yaml\")\n",
    "\n",
    "checkpoint_url = \"https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1\"\n",
    "config_url = \"https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1\"\n",
    "\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    urllib.request.urlretrieve(checkpoint_url,checkpoint_path)\n",
    "else:\n",
    "    print(\"Last.ckpt already exists\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    urllib.request.urlretrieve(config_url, config_path)\n",
    "else:\n",
    "    print(\"model.yaml already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## install some extra libraries\n",
    "# !pip install --no-deps ftfy regex tqdm\n",
    "# !pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
    "# !pip uninstall torchtext --yes\n",
    "# !pip install einops\n",
    "# !pip install openai-clip\n",
    "# !pip install taming-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, imageio, pdb, math\n",
    "import torchvision.transforms \n",
    "import torchvision.transforms.functional \n",
    "\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def show_from_tensor(tensor):\n",
    "    img = tensor.clone()\n",
    "    img = img.mul(255).byte()\n",
    "    img = img.cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def save_tensor(tensor, path):\n",
    "    img = tensor.clone()\n",
    "    img = img.mul(255).byte()\n",
    "    img = img.cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    plt.savefig(path)\n",
    "    \n",
    "def norm_data(data):\n",
    "    return (data.clip(-1, 1) + 1) / 2\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.5\n",
    "batch_size = 1\n",
    "weight_decay = 0.1\n",
    "noise_factor = 0.22\n",
    "\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "\n",
    "total_iteration = 100\n",
    "img_shape = [225, 400, 3]  # height, width, channel\n",
    "size1, size2, channels = img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP MODEL\n",
    "clipmodel, _ = clip.load(\"ViT-B/32\", jit=False)\n",
    "clipmodel.eval()\n",
    "print(clip.available_models())\n",
    "\n",
    "print(\"Clip model visual input resolution:\", clipmodel.visual.input_resolution)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taming.models.vqgan import VQModel\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "    config_data = OmegaConf.load(config_path)\n",
    "    if display:\n",
    "        print(yaml.dump(OmegaConf.to_container(config_data)))\n",
    "    return config_data\n",
    "\n",
    "def load_vqgan(config, chk_path=None):\n",
    "    model = VQModel(**config.model.params)\n",
    "    if chk_path is not None:\n",
    "        state_dict = torch.load(chk_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    return model.eval()\n",
    "\n",
    "def generator(x):\n",
    "    x = taming_model.post_quant_conv(x)\n",
    "    x = taming_model.decoder(x)\n",
    "    return x\n",
    "\n",
    "taming_config = load_config(config_path=config_path)\n",
    "taming_model = load_vqgan(config=taming_config, chk_path=checkpoint_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Declare the values that we are going to optimize\n",
    "class Parameters(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Parameters,self).__init__()\n",
    "        self.data = 0.5*torch.randn(batch_size,256,size1//16, size2//16).cuda()\n",
    "        self.data = torch.nn.Parameter(torch.sin(self.data))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.data\n",
    "\n",
    "def init_params():\n",
    "  params=Parameters().cuda()\n",
    "  optimizer = torch.optim.AdamW([{'params':[params.data], 'lr': learning_rate}], weight_decay=weight_decay)\n",
    "  return params, optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encodings, prompts and ....\n",
    "normalize = torchvision.transforms.Normalize(\n",
    "    (0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
    ")\n",
    "\n",
    "def encodeText(text):\n",
    "    t = clip.tokenize(text).cuda()\n",
    "    t = clipmodel.encode_text(t).detach().clone()\n",
    "    return t\n",
    "\n",
    "def createEncodings(include, exclude, extras):\n",
    "    include_enc = []\n",
    "    for text in include:\n",
    "        include_enc.append(encodeText(text))\n",
    "    exclude_enc = encodeText(exclude) if exclude != \"\" else 0\n",
    "    extras_enc = encodeText(extras) if extras != \"\" else 0\n",
    "    \n",
    "    return include_enc, exclude_enc, extras_enc\n",
    "\n",
    "augTransform = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomAffine(30,(0.2,0.2), fill=0)\n",
    ").cuda()\n",
    "\n",
    "Params, optimizer = init_params()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(Params().shape)\n",
    "    img = norm_data(generator(Params()).cpu())\n",
    "    print(\"img dimension: \", img.shape)\n",
    "    show_from_tensor(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create crops\n",
    "def create_crops(img, num_crops=30):\n",
    "    p=size1//2\n",
    "    img = torch.nn.functional.pad(img, (p,p,p,p), mode='constant', value=0) # 1 x 3 x 448 x 624\n",
    "\n",
    "    img = augTransform(img) #RandomHorizontalFilp and RandomAffine\n",
    "\n",
    "    crop_set = []\n",
    "    for ch in range(num_crops):\n",
    "        gap1 = int(torch.normal(1.0,0.5,()).clip(0.2,1.5)*size1)\n",
    "        offsetx = torch.randint(0, int(size1*2 - gap1),())\n",
    "        offsety = torch.randint(0, int(size1*2 - gap1), ())\n",
    "        \n",
    "        crop = img[:,:,offsetx:offsetx+gap1, offsety:offsety+gap1]\n",
    "        \n",
    "        crop = torch.nn.functional.interpolate(crop,(224,224),mode=\"bilinear\",align_corners=True)\n",
    "        crop_set.append(crop)\n",
    "    img_crops = torch.cat(crop_set,0)\n",
    "    \n",
    "    randnormal = torch.randn_like(img_crops,requires_grad=False)\n",
    "    num_rands = 0\n",
    "    randstotal = torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
    "    for ns in range(num_rands):\n",
    "        randstotal*= torch.rand((img_crops.shape[0],1,1,1)).cuda()\n",
    "    \n",
    "    img_crops = img_crops + noise_factor * randstotal * randnormal\n",
    "    \n",
    "    return img_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showme(Params, show_crop):\n",
    "    with torch.no_grad():\n",
    "        generated = generator(Params())\n",
    "        \n",
    "        if show_crop:\n",
    "            print(\"Augmented cropped example\")\n",
    "            aug_gen = generated.float()\n",
    "            aug_gen = create_crops(aug_gen, num_crops=1)\n",
    "            aug_gen_norm = norm_data(aug_gen[0])\n",
    "            show_from_tensor(aug_gen_norm)\n",
    "            \n",
    "        print(\"Generation\")\n",
    "        latest_gen = norm_data(generated.cpu())\n",
    "        show_from_tensor(latest_gen[0])\n",
    "        \n",
    "    return (latest_gen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization Process\n",
    "def optimize_result(Params, prompt,extras_enc, exclude_enc):\n",
    "    alpha, beta = 1, 0.5 #alpha: importance of include encodings beta:importance of exclude\n",
    "    \n",
    "    #image encoding\n",
    "    out = generator(Params())\n",
    "    out = norm_data(out)\n",
    "    out = create_crops(out)\n",
    "    out = normalize(out)\n",
    "    image_enc = clipmodel.encode_image(out)\n",
    "    \n",
    "    #text encoding\n",
    "    final_enc = w1*prompt + w1*extras_enc #1 x 512\n",
    "    final_text_include_enc = final_enc / final_enc.norm(dim=1, keepdim=True)\n",
    "    final_text_exclude_enc = exclude_enc\n",
    "    \n",
    "    #Calculate the loss\n",
    "    main_loss = torch.cosine_similarity(final_text_include_enc, image_enc, -1)# 30\n",
    "    penalize_loss = torch.cosine_similarity(final_text_exclude_enc, image_enc, -1) #30\n",
    "    \n",
    "    final_loss = -alpha*main_loss + beta*penalize_loss\n",
    "    \n",
    "    return final_loss\n",
    "    \n",
    "def optimize(Params, optimizer, prompt, extras_enc, exclude_enc):\n",
    "    loss = optimize_result(Params,prompt, extras_enc, exclude_enc).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(Params, optimizer, include_enc, exclude_enc, extras_enc, show_step=(total_iteration-1), show_crop=False):\n",
    "    res_img=[]\n",
    "    res_z=[]\n",
    "    \n",
    "    for prompt in include_enc:\n",
    "        iteration=0\n",
    "        # Params, optimizer = init_params() #1 x 256 x 14 x 25\n",
    "        \n",
    "        for i in range(total_iteration):\n",
    "            loss = optimize(Params, optimizer, prompt, exclude_enc, extras_enc)\n",
    "            \n",
    "            if iteration >= (total_iteration//2) and iteration % show_step == 0:\n",
    "                new_img = showme(Params, show_crop)\n",
    "                res_img.append(new_img)\n",
    "                res_z.append(Params())\n",
    "                print(\"loss:\",loss.item(), \"\\niteration:\", iteration)\n",
    "                \n",
    "            iteration+=1\n",
    "        torch.cuda.empty_cache()\n",
    "    return res_img, res_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "include = [\"sketch of a lady\", \"sketch of a man on a horse\"]\n",
    "exclude = ' watermark, cropped, confusing, incoherent, cut, blurry'\n",
    "extras = \"watercolor paper texture\"\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "\n",
    "include_enc, exclude_enc, extras_enc = createEncodings(include, exclude, extras)\n",
    "res_img, res_z = training_loop(Params, optimizer, include_enc, exclude_enc, extras_enc, show_crop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, img in zip(include, res_img):\n",
    "    save_tensor(img,(prompt+\".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(res_img), len(res_z))\n",
    "print(res_img[0].shape, res_z[0].shape)\n",
    "print(res_z[0].max(), res_z[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(res_z_list, duration_list):\n",
    "    gen_img_list=[]\n",
    "    fps=25\n",
    "    \n",
    "    for idx, (z, duration) in enumerate(res_z_list, duration_list):\n",
    "        num_steps = int(duration*fps)\n",
    "        z1 = z\n",
    "        z2 = res_z_list[(idx+1)%len(res_z_list)]\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            alpha = math.sin(1.5*step/num_steps)**6\n",
    "            z_new = alpha *z2 + (1-alpha)*z1\n",
    "            \n",
    "            new_gen = norm_data(generator(z_new).cpu())[0]\n",
    "            new_img = torchvision.transforms.ToPILImage(mode='RGB')(new_gen)\n",
    "            gen_img_list.append(new_img)\n",
    "            \n",
    "    return gen_img_list\n",
    "\n",
    "durations = [3,3,3,3,3,3]\n",
    "interpolate_results_images = interpolate(res_z, durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_video_path=f\"res1.mp4\"\n",
    "writer = imageio.get_writer(out_video_path, fps=25)\n",
    "for pil_img in interpolate_results_images:\n",
    "    img = np.array(pil_img, dtype=np.uint8)\n",
    "    writer.append_data(img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open(f\"res1.mp4\", \"rb\").read()\n",
    "data = \"data:video/mp4;base64,\"+b64encode(mp4).decode()\n",
    "HTML(\"\"\"<video width=800 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\"%data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
